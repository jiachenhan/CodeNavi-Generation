from __future__ import annotations

"""
dashboard_builder.py

å°è£…å¯è§†åŒ–æ‰€éœ€çš„æ•°æ®åˆå¹¶ä¸é¡µé¢æ„å»ºé€»è¾‘ã€‚
"""

import json
from pathlib import Path

from jinja2 import Environment, FileSystemLoader

OUTPUT_SUBDIR = "output"


def map_dataset_name(dataset_name: str) -> str:
    """
    å°†æ•°æ®é›†åç§°æ˜ å°„åˆ°ä»£ç å¯¹æ•°æ®é›†ç›®å½•ä¸­çš„åç§°ã€‚
    æå–å·¥å…·åç§°å¹¶æ˜ å°„ï¼š
    - codeql_v1_commits / codeql_v2_commits -> ql
    - pmd_v1_commits / pmd_v2_commits -> pmd
    """
    # è§£ææ ¼å¼ï¼š{tool}_{version}_commits
    parts = dataset_name.split("_")
    if len(parts) >= 2:
        tool = parts[0]
        mapping = {
            "codeql": "ql",
            "pmd": "pmd",
        }
        return mapping.get(tool, tool)
    return dataset_name


def get_output_dir(work_dir: Path) -> Path:
    """è¿”å›ç”¨äºå­˜æ”¾ç”Ÿæˆæ–‡ä»¶çš„ç›®å½•ï¼ˆä¾‹å¦‚ work_dir/outputï¼‰ã€‚"""
    return work_dir.resolve() / OUTPUT_SUBDIR


def merge_results_from_dirs(
    base_dir: Path,
    dsl_base_dir: Path | None = None,
    code_pair_base_dir: Path | None = None,
) -> list[dict]:
    """
    åˆå¹¶ base_dir ä¸‹æ‰€æœ‰æ•°æ®é›†çš„æ ‡æ³¨ç»“æœï¼Œè¿”å›ç»Ÿä¸€çš„åˆ—è¡¨ã€‚

    ç›®å½•å±‚çº§çº¦å®šï¼š
      base_dir / {dataset} / {checker} / {group} / *_labeled_results.json
    """
    results: list[dict] = []
    base_dir = base_dir.resolve()
    dsl_base_dir = dsl_base_dir.resolve() if dsl_base_dir is not None else None
    code_pair_base_dir = code_pair_base_dir.resolve() if code_pair_base_dir is not None else None

    # ç¼“å­˜ DSL æ–‡ä»¶å†…å®¹ï¼Œé¿å…é‡å¤è¯»å–
    dsl_cache: dict[Path, str] = {}
    # ç¼“å­˜ä»£ç å¯¹å’Œ info.jsonï¼Œé¿å…é‡å¤è¯»å–
    code_pair_cache: dict[Path, dict] = {}

    for dataset_dir in base_dir.iterdir():
        if not dataset_dir.is_dir():
            continue
        for checker_dir in dataset_dir.iterdir():
            if not checker_dir.is_dir():
                continue
            for group_dir in checker_dir.iterdir():
                if not group_dir.is_dir():
                    continue
                labeled_files = list(group_dir.glob("*_labeled_results.json"))
                if not labeled_files:
                    continue

                for labeled_file in labeled_files:

                    def get_case_info(case_file_name: str) -> str:
                        parts = case_file_name.split("_")
                        if len(parts) == 4 and parts[2] == "labeled" and parts[3] == "results":
                            dsl_case, scanned_case, *_ = parts
                            return f"{dsl_case}_{scanned_case}"
                        raise ValueError(f"Unexpected dataset name format: {case_file_name}")

                    with labeled_file.open("r", encoding="utf-8") as f:
                        items = json.load(f)

                    # æ¯ä¸ª labeled_file å¯¹åº”ä¸€ä¸ª caseInfoï¼Œå¯ä»¥ç”¨æ¥æ¨å¯¼ DSL æ–‡ä»¶å
                    case_info = get_case_info(labeled_file.stem)
                    case_num = case_info.split("_", 1)[0]

                    # è¯»å– DSL æ–‡ä»¶
                    dsl_source: str | None = None
                    if dsl_base_dir is not None:
                        dsl_file = (
                            dsl_base_dir
                            / dataset_dir.name
                            / checker_dir.name
                            / group_dir.name
                            / f"{case_num}.kirin"
                        )
                        if dsl_file in dsl_cache:
                            dsl_source = dsl_cache[dsl_file]
                        elif dsl_file.is_file():
                            with dsl_file.open("r", encoding="utf-8") as df:
                                dsl_source = df.read()
                            dsl_cache[dsl_file] = dsl_source

                    # è¯»å–ä»£ç å¯¹å’Œ info.jsonï¼ˆå¯¹åº” case1ï¼‰
                    buggy_code: str | None = None
                    fixed_code: str | None = None
                    may_be_fixed_violations: str | None = None
                    if code_pair_base_dir is not None:
                        mapped_dataset = map_dataset_name(dataset_dir.name)
                        code_pair_case_dir = (
                            code_pair_base_dir
                            / mapped_dataset
                            / checker_dir.name
                            / group_dir.name
                            / case_num
                        )
                        
                        if code_pair_case_dir in code_pair_cache:
                            cached = code_pair_cache[code_pair_case_dir]
                            buggy_code = cached.get("buggy_code")
                            fixed_code = cached.get("fixed_code")
                            may_be_fixed_violations = cached.get("may_be_fixed_violations")
                        elif code_pair_case_dir.is_dir():
                            # è¯»å– buggy.java
                            buggy_file = code_pair_case_dir / "buggy.java"
                            if buggy_file.is_file():
                                with buggy_file.open("r", encoding="utf-8") as f:
                                    buggy_code = f.read()
                            
                            # è¯»å– fixed.java
                            fixed_file = code_pair_case_dir / "fixed.java"
                            if fixed_file.is_file():
                                with fixed_file.open("r", encoding="utf-8") as f:
                                    fixed_code = f.read()
                            
                            # è¯»å– info.json
                            info_file = code_pair_case_dir / "info.json"
                            if info_file.is_file():
                                with info_file.open("r", encoding="utf-8") as f:
                                    info_data = json.load(f)
                                    may_be_fixed_violations = info_data.get("may_be_fixed_violations", "")
                            
                            # ç¼“å­˜ç»“æœ
                            code_pair_cache[code_pair_case_dir] = {
                                "buggy_code": buggy_code,
                                "fixed_code": fixed_code,
                                "may_be_fixed_violations": may_be_fixed_violations,
                            }

                    for item in items:
                        item["dataset"] = dataset_dir.name
                        item["checker"] = checker_dir.name
                        item["group"] = group_dir.name
                        item["case_info"] = case_info
                        if dsl_source is not None:
                            item["dsl_source"] = dsl_source
                        if buggy_code is not None:
                            item["buggy_code"] = buggy_code
                        if fixed_code is not None:
                            item["fixed_code"] = fixed_code
                        if may_be_fixed_violations is not None:
                            item["may_be_fixed_violations"] = may_be_fixed_violations
                        results.append(item)

    return results


def generate_dashboard(data: list[dict], work_dir: Path, output_html: Path | None = None) -> Path:
    """
    åŸºäºä¼ å…¥çš„æ•°æ®ç”Ÿæˆ data.json ä¸ HTML é¡µé¢ã€‚

    - data.json å†™åœ¨ output_dir / data.json
    - dashboard.html é»˜è®¤å†™åœ¨ output_dir / dashboard.htmlï¼Œæˆ–ä½¿ç”¨ output_html è¦†ç›–
    - è¿”å›æœ€ç»ˆ HTML çš„è·¯å¾„
    """
    work_dir = work_dir.resolve()
    output_dir = get_output_dir(work_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    templates_dir = work_dir / "templates"

    env = Environment(loader=FileSystemLoader(templates_dir))
    template = env.get_template("dashboard_template.html")

    # å†™å…¥ data.json
    data_file_path = output_dir / "data.json"
    with data_file_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # æ¸²æŸ“ HTML
    html_content = template.render()
    output_path = output_html or (output_dir / "dashboard.html")
    with output_path.open("w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"âœ… data.json å·²ç”Ÿæˆ: {data_file_path}")
    print(f"âœ… å¯è§†åŒ–é¡µé¢å·²ç”Ÿæˆ: {output_path}")
    return output_path


def prepare_dashboard(
    base_dir: Path,
    work_dir: Path,
    dsl_base_dir: Path | None = None,
    code_pair_base_dir: Path | None = None,
) -> Path:
    """
    ä¸€æ­¥å®Œæˆæ•°æ®åˆå¹¶ + dashboard ç”Ÿæˆï¼Œä¾›æœåŠ¡å¯åŠ¨æ—¶è°ƒç”¨ã€‚

    è¿”å›ç”Ÿæˆçš„ HTML è·¯å¾„ã€‚
    """
    base_dir = base_dir.resolve()
    work_dir = work_dir.resolve()

    print(f"ğŸ” æ­£åœ¨ä»ç»“æœç›®å½•æ”¶é›†æ•°æ®: {base_dir}")
    if dsl_base_dir is not None:
        print(f"ğŸ§¾ DSL ä»£ç ç›®å½•: {dsl_base_dir}")
    if code_pair_base_dir is not None:
        print(f"ğŸ“ ä»£ç å¯¹æ•°æ®é›†ç›®å½•: {code_pair_base_dir}")
    data = merge_results_from_dirs(base_dir, dsl_base_dir=dsl_base_dir, code_pair_base_dir=code_pair_base_dir)
    print(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(data)} æ¡å‡½æ•°çº§åˆ«è®°å½•")

    return generate_dashboard(data, work_dir)

